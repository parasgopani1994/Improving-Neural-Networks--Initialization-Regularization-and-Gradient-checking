This repository contains practise notebooks on Improving neural networks by different ways of initialization and regularization. Some methods of weight initializations discussed are random initialization, He initialization and zeros initialization. He initializations gave the best results. 
For regularization, L2 and dropout regularization has been discussed to overcome overfitting in neural network models.
Finally, gradient checking is discussed where you can check if the backward propagation carried out by your optimization method was accurate or not and how ot find any bugs if not.
